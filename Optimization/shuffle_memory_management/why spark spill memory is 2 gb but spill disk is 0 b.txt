In Apache Spark, the terms "spill memory" and "spill disk" are related to how Spark handles data that doesn't fit in memory during computation. The behavior you're describing—where "spill memory" is 2 GB but "spill disk" is 0 B—suggests that Spark has encountered more data than it can hold in memory but hasn't yet spilled the data to disk. Here's a breakdown of what could be happening:

### 1. **Memory Spill Behavior**:
   - **Spill Memory** refers to the amount of data that Spark is able to hold temporarily in memory, but exceeds the available memory for storing intermediate results. When Spark’s shuffle memory or data processing memory is full (but still below the threshold for spilling to disk), it will keep this data in a portion of memory that is allocated for this purpose.
   
   - The **2 GB spill memory** means that Spark has utilized 2 GB of memory to store intermediate data, but it hasn't yet needed to spill it to disk because Spark is still managing it within its memory resources.

### 2. **Disk Spill Behavior**:
   - **Spill Disk** refers to the actual amount of data that Spark has written to disk because the data couldn’t fit in memory. However, Spark doesn’t spill data to disk immediately when memory is exhausted; it might first try to utilize other mechanisms, such as:
     - **Tuning the in-memory storage**: Spark will try to store data in memory until the configured memory limits are exceeded, and only then will it spill over to disk.
     - **Internal mechanisms for spilling**: Spark may use a **memory manager** to control how much data is spilled to disk, and it may not immediately spill data until it absolutely has to.

   - The **0 B spill disk** means that, even though 2 GB of data has been stored in spill memory, it hasn’t yet reached the point where Spark needs to write this data to disk. This could be due to one or more of the following:
     - **Sufficient memory available**: If your cluster has enough memory available or has been properly tuned, Spark may avoid spilling data to disk.
     - **Memory configurations**: Certain Spark configurations control how aggressively Spark spills data. For instance, `spark.shuffle.memoryFraction` controls how much of the heap is used for shuffle operations and can influence when data is spilled to disk.
     - **Disk spill thresholds**: If the threshold for spilling to disk is high or if the memory allocation is sufficiently large, Spark might not spill data to disk even though spill memory is in use.

### 3. **Possible Configurations Affecting This Behavior**:
   - `spark.memory.fraction`: Determines the fraction of heap space used for execution and storage (default is 0.6). Increasing this value gives Spark more memory to work with and may delay spilling.
   - `spark.shuffle.spill.compress`: If enabled, this can reduce the amount of disk space required to spill data, though it can still affect whether disk spill happens.
   - `spark.shuffle.spill.initialFileSize`: Controls the threshold at which Spark will begin spilling shuffle data to disk.
   - `spark.executor.memory`: The total amount of memory allocated for each executor. If this is set large enough, Spark may hold more data in memory and avoid spilling to disk.

### 4. **When Spill to Disk Happens**:
   - Spill to disk typically happens only when the memory allocated for shuffle data or cached data is full, and further data can't be stored in memory.
   - If you see spill memory growing (2 GB) without a corresponding increase in disk spill (0 B), it could indicate that Spark is still processing data within the allocated memory space and hasn’t yet hit the threshold to trigger disk spilling.

### 5. **Optimizing for Spill**:
   - If you want to reduce spill to disk or tune how Spark manages spill memory, you can:
     - Adjust Spark's memory settings to allocate more memory to executors or adjust how much memory is used for shuffle operations.
     - Consider enabling disk-based shuffle or adjusting the thresholds at which data is spilled.

In summary, the 2 GB of "spill memory" suggests that Spark is keeping data in memory for shuffle or processing, but has not yet needed to spill it to disk. If the disk spill is still 0 B, Spark may not have encountered memory pressure severe enough to trigger disk spilling, or it may be managing memory efficiently enough to avoid spilling to disk for now.